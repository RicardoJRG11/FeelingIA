{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V7K_QWtKA4z"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import classify\n",
        "from nltk import NaiveBayesClassifier"
      ],
      "metadata": {
        "id": "x1CJ--CcLL8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "gpSnxb7Sdm9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_tweet(tweet):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stopwords_set = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "A0chwgrfRekv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convertir a minúsculas#\n",
        "tweet = tweet.lower()\n",
        "#Tokenizar el tweet#\n",
        "tokens = word_tokenize(tweet)\n",
        "#Eliminar los caracteres no alfanuméricos#\n",
        "tokens = [token for token in tokens if token.isalpha()]\n",
        "#Lemmatización#\n",
        "tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "#Eliminar las stopwords#\n",
        "tokens = [token for token in tokens if token not in stopwords_set]"
      ],
      "metadata": {
        "id": "Yfiwz3XERoJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "return tokens\n",
        "def get_features(tweet):\n",
        "  features = {}\n",
        "  \n",
        "  for word in tweet:\n",
        "    features[word] = True\n",
        "\n",
        "return features"
      ],
      "metadata": {
        "id": "MjA8X6t3TrRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')"
      ],
      "metadata": {
        "id": "zzeKCuXDUqQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')"
      ],
      "metadata": {
        "id": "ThQbOvK8WD7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []"
      ],
      "metadata": {
        "id": "JLbF1BNPWHSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(preprocess_tweet(tokens))"
      ],
      "metadata": {
        "id": "Jo-C3-hfaane"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(preprocess_tweet(tokens))"
      ],
      "metadata": {
        "id": "rfW2u11lahFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets_features = []\n",
        "negative_tweets_features = []"
      ],
      "metadata": {
        "id": "NsfaNkNxaplD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet_tokens in positive_cleaned_tokens_list:\n",
        "    positive_tweets_features.append((get_features(tweet_tokens), 'Positive'))"
      ],
      "metadata": {
        "id": "9_gKMA1_bGqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for tweet_tokens in negative_cleaned_tokens_list:\n",
        "    negative_tweets_features.append((get_features(tweet_tokens), 'Negative'))\n",
        "    positive_cutoff = int(len(positive_tweets_features) * 0.8)\n",
        "negative_cutoff = int(len(negative_tweets_features) * 0.8)"
      ],
      "metadata": {
        "id": "-eP-9PTjcQBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = positive_tweets_features[:positive_cutoff] + negative_tweets_features[:negative_cutoff]\n",
        "test_features = positive_tweets_features[positive_cutoff:] + negative_tweets_features[negative_cutoff:]\n",
        "classifier = NaiveBayesClassifier.train(train_features)\n",
        "accuracy = classify.accuracy(classifier, test_features)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "cMwsi08Vcfa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = \"I love this product!\"\n",
        "custom_tokens = preprocess_tweet(word_tokenize(custom_tweet))\n",
        "print(custom_tweet, classifier.classify(get_features(custom_tokens)))"
      ],
      "metadata": {
        "id": "GmWTjcxidcWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "custom_tweet = \"I hate this product!\"\n",
        "custom_tokens = preprocess_tweet(word_tokenize(custom_tweet))\n",
        "print(custom_tweet, classifier.classify(get_features(custom_tokens)))"
      ],
      "metadata": {
        "id": "mXfomx3YdieE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}